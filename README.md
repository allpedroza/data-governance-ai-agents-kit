# Data Governance AI Agents Kit

**Framework completo de agentes de IA para governanÃ§a de dados**, fornecendo anÃ¡lise de linhagem, descoberta semÃ¢ntica, enriquecimento de metadados, classificaÃ§Ã£o de dados, monitoramento de qualidade, anÃ¡lise de valor de ativos e proteÃ§Ã£o contra vazamento de dados sensÃ­veis.

## VisÃ£o Geral

Este projeto fornece **7 agentes de IA especializados** que trabalham de forma integrada para resolver desafios de governanÃ§a de dados:

| Agente | PropÃ³sito |
|--------|-----------|
| **Data Lineage Agent** | Mapear dependÃªncias e analisar impacto de mudanÃ§as |
| **Data Discovery RAG Agent** | Descoberta semÃ¢ntica de dados com busca em linguagem natural |
| **Metadata Enrichment Agent** | GeraÃ§Ã£o automÃ¡tica de descriÃ§Ãµes, tags e classificaÃ§Ãµes |
| **Data Classification Agent** | ClassificaÃ§Ã£o de PII/PHI/Financeiro/ e termos sensÃ­veis Ã  estratÃ©gia dos negÃ³cios a partir de metadados |
| **Data Quality Agent** | Monitoramento de qualidade com SLA e detecÃ§Ã£o de schema drift |
| **Data Asset Value Agent** | AnÃ¡lise de valor de ativos baseado em uso, JOINs e impacto em data products |
| **Sensitive Data NER Agent** | DetecÃ§Ã£o e anonimizaÃ§Ã£o de dados sensÃ­veis em texto livre para proteÃ§Ã£o de LLMs |

## InÃ­cio RÃ¡pido

### Com uv (Recomendado)

[uv](https://docs.astral.sh/uv/) Ã© um gerenciador de pacotes Python extremamente rÃ¡pido (10-100x mais rÃ¡pido que pip).

```bash
# Instale o uv (se ainda nÃ£o tiver)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Clone o repositÃ³rio
git clone <repo-url>
cd data-governance-ai-agents-kit

# Crie ambiente virtual e instale dependÃªncias
uv venv
source .venv/bin/activate  # Linux/macOS
# .venv\Scripts\activate   # Windows

uv pip install -r requirements.txt

# Configure a API key (necessÃ¡ria para alguns agentes)
export OPENAI_API_KEY="sua-chave-aqui"

# Inicie a interface unificada
streamlit run app.py
```

### Com pip (Alternativo)

```bash
# Clone o repositÃ³rio
git clone <repo-url>
cd data-governance-ai-agents-kit

# Crie ambiente virtual (opcional, mas recomendado)
python -m venv .venv
source .venv/bin/activate  # Linux/macOS
# .venv\Scripts\activate   # Windows

# Instale as dependÃªncias
pip install -r requirements.txt

# Configure a API key (necessÃ¡ria para alguns agentes)
export OPENAI_API_KEY="sua-chave-aqui"

# Inicie a interface unificada
streamlit run app.py
```

---

## Agentes DisponÃ­veis

### 1. Data Lineage Agent

Sistema de IA para **anÃ¡lise automÃ¡tica de linhagem de dados** em pipelines complexos.

**CaracterÃ­sticas**:
- AnÃ¡lise de mÃºltiplos formatos (Python, SQL, Terraform, Databricks, Airflow, Scala)
- ExtraÃ§Ã£o automÃ¡tica de dependÃªncias entre assets
- VisualizaÃ§Ã£o interativa de grafos (Force, Hierarchical, Sankey, 3D)
- AnÃ¡lise de impacto de mudanÃ§as
- IdentificaÃ§Ã£o de componentes crÃ­ticos e ciclos
- IntegraÃ§Ã£o com Apache Atlas

**DocumentaÃ§Ã£o**: [lineage/README.md](lineage/README.md)

**Exemplo**:
```python
from lineage.data_lineage_agent import DataLineageAgent

agent = DataLineageAgent()
analysis = agent.analyze_pipeline([
    "etl/extract.sql",
    "etl/transform.py",
    "etl/load.sql"
])

# AnÃ¡lise de impacto
impact = agent.analyze_change_impact(["customers_table"])
print(f"Risk Level: {impact['risk_level']}")
```

---

### 2. Data Discovery RAG Agent

Sistema de IA para **descoberta de dados** usando **RAG (Retrieval-Augmented Generation)** com busca hÃ­brida (semÃ¢ntica + lexical).

**CaracterÃ­sticas**:
- Busca semÃ¢ntica em linguagem natural
- Dartboard Ranking (semÃ¢ntica + lexical + importÃ¢ncia)
- ValidaÃ§Ã£o de tabelas contra catÃ¡logo
- Providers plugÃ¡veis (OpenAI, SentenceTransformers, VertexAI)
- Vector stores: ChromaDB, FAISS
- IntegraÃ§Ã£o com Apache Atlas e Lineage Agent

**DocumentaÃ§Ã£o**: [rag_discovery/README.md](rag_discovery/README.md)

**Exemplo**:
```python
from rag_discovery.agent import DataDiscoveryAgent
from rag_discovery.providers.embeddings import SentenceTransformerEmbeddings
from rag_discovery.providers.llm import OpenAILLM
from rag_discovery.providers.vectorstore import ChromaStore

# Inicializa com providers
agent = DataDiscoveryAgent(
    embedding_provider=SentenceTransformerEmbeddings(),
    llm_provider=OpenAILLM(),
    vector_store=ChromaStore(collection_name="my_catalog")
)

# Indexa metadados
agent.index_from_json("catalog.json")

# Busca semÃ¢ntica
result = agent.discover("Onde estÃ£o os dados de clientes?")
print(result.answer)
```

---

### 3. Metadata Enrichment Agent

Sistema de IA para **geraÃ§Ã£o automÃ¡tica de metadados** usando RAG sobre padrÃµes de arquitetura e sampling de dados.

**CaracterÃ­sticas**:
- GeraÃ§Ã£o de descriÃ§Ãµes para tabelas e colunas (PT-BR e EN)
- ClassificaÃ§Ã£o automÃ¡tica de dados (public, internal, confidential, restricted)
- DetecÃ§Ã£o de PII (CPF, CNPJ, email, telefone, etc.)
- RAG sobre normativos e padrÃµes de nomenclatura
- SugestÃ£o de domÃ­nio e proprietÃ¡rio
- Suporte a CSV, Parquet, SQL, Delta Lake

**DocumentaÃ§Ã£o**: [metadata_enrichment/README.md](metadata_enrichment/README.md)

**Exemplo**:
```python
from metadata_enrichment.agent import MetadataEnrichmentAgent
from rag_discovery.providers.embeddings import SentenceTransformerEmbeddings
from rag_discovery.providers.llm import OpenAILLM
from rag_discovery.providers.vectorstore import ChromaStore

agent = MetadataEnrichmentAgent(
    embedding_provider=SentenceTransformerEmbeddings(),
    llm_provider=OpenAILLM(model="gpt-4o-mini"),
    vector_store=ChromaStore(collection_name="standards")
)

# Indexar padrÃµes de nomenclatura
agent.index_standards_from_json("standards.json")

# Enriquecer metadados
result = agent.enrich_from_csv("customers.csv")

print(f"DescriÃ§Ã£o: {result.description}")
print(f"PII detectado: {result.has_pii}")
print(f"Colunas PII: {result.pii_columns}")
```

---

### 4. Data Classification Agent

Sistema de IA para **classificaÃ§Ã£o automÃ¡tica de dados** por nÃ­veis de sensibilidade, detectando PII, PHI, PCI, dados financeiros e termos estratÃ©gicos proprietÃ¡rios.

**CaracterÃ­sticas**:
- DetecÃ§Ã£o de PII (CPF, CNPJ, SSN, email, telefone, IP, etc.)
- DetecÃ§Ã£o de PHI (CID-10, CNS, CRM, prontuÃ¡rio mÃ©dico)
- DetecÃ§Ã£o de PCI (cartÃ£o de crÃ©dito, CVV, IBAN, SWIFT)
- DetecÃ§Ã£o de dados financeiros (contas, transaÃ§Ãµes, valores)
- DetecÃ§Ã£o de termos estratÃ©gicos proprietÃ¡rios via **dicionÃ¡rio customizÃ¡vel** (nomes de projetos, iniciativas, roadmaps)
- NÃ­veis de sensibilidade: public, internal, confidential, restricted
- Flags de compliance: LGPD, GDPR, HIPAA, PCI-DSS, SOX
- Suporte a CSV, Parquet, SQL, Delta Lake

**DocumentaÃ§Ã£o**: [data_classification/README.md](data_classification/README.md)

**Exemplo**:
```python
from data_classification import DataClassificationAgent

agent = DataClassificationAgent()

# Classificar dados
report = agent.classify_from_csv("customers.csv")

print(f"Sensibilidade: {report.overall_sensitivity}")
print(f"PII: {report.pii_columns}")
print(f"PHI: {report.phi_columns}")
print(f"ProprietÃ¡rio: {report.proprietary_columns}")
print(f"Compliance: {report.compliance_flags}")
```

---

### 5. Data Quality Agent

Sistema de IA para **monitoramento de qualidade de dados** com mÃ©tricas multi-dimensionais, SLA e detecÃ§Ã£o de schema drift.

**CaracterÃ­sticas**:
- 6 dimensÃµes de qualidade: Completeness, Uniqueness, Validity, Consistency, Freshness, Schema
- Monitoramento de Freshness com SLA configurÃ¡vel
- DetecÃ§Ã£o de schema drift com versionamento
- Sistema de regras e alertas configurÃ¡veis
- Suporte a CSV, Parquet, SQL, Delta Lake
- ExportaÃ§Ã£o de relatÃ³rios (JSON, Markdown)

**DocumentaÃ§Ã£o**: [data_quality/README.md](data_quality/README.md)

**Exemplo**:
```python
from data_quality.agent import DataQualityAgent
from data_quality.rules import QualityRule, AlertLevel

agent = DataQualityAgent(enable_schema_tracking=True)

# Avaliar qualidade com SLA de freshness
report = agent.evaluate_file(
    "orders.parquet",
    freshness_config={
        "timestamp_column": "updated_at",
        "sla_hours": 4
    },
    validity_configs=[{
        "column": "email",
        "pattern_name": "email",
        "threshold": 0.95
    }]
)

print(f"Score: {report.overall_score:.0%}")
print(f"Status: {report.overall_status}")
print(f"Schema Drift: {report.schema_drift}")
```

---

### 6. Data Asset Value Agent

Sistema de IA para **anÃ¡lise de valor de ativos de dados** baseado em padrÃµes de uso, relacionamentos de JOIN, linhagem e impacto em data products.

**CaracterÃ­sticas**:
- AnÃ¡lise de uso direto em queries SQL
- Mapeamento de relacionamentos de JOIN (hub assets)
- IntegraÃ§Ã£o com saÃ­da do Data Lineage Agent
- Scoring multi-dimensional de valor (uso, JOINs, linhagem, data products)
- IdentificaÃ§Ã£o de ativos crÃ­ticos e Ã³rfÃ£os
- DetecÃ§Ã£o de tendÃªncias de uso (increasing/stable/decreasing)
- RecomendaÃ§Ãµes de governanÃ§a automatizadas
- Suporte a configuraÃ§Ã£o de data products com impacto de negÃ³cio

**DocumentaÃ§Ã£o**: [data_asset_value/README.md](data_asset_value/README.md)

**Exemplo**:
```python
from data_asset_value import DataAssetValueAgent

agent = DataAssetValueAgent(
    weights={
        'usage': 0.30,       # FrequÃªncia de uso
        'joins': 0.25,       # Relacionamentos de JOIN
        'lineage': 0.25,     # Impacto na linhagem
        'data_product': 0.20 # ImportÃ¢ncia em data products
    },
    time_range_days=30
)

# Analisar a partir de logs de queries
report = agent.analyze_from_query_logs(
    query_logs=query_logs,
    lineage_data=lineage_agent_output,  # IntegraÃ§Ã£o com Lineage Agent
    data_product_config=data_products,
    asset_metadata=asset_metadata
)

# Top assets por valor
for asset in report.asset_scores[:5]:
    print(f"{asset.asset_name}: {asset.overall_value_score:.1f} ({asset.value_category})")

# Ativos crÃ­ticos e hubs
print(f"CrÃ­ticos: {report.critical_assets}")
print(f"Hubs: {report.hub_assets}")
print(f"Ã“rfÃ£os: {report.orphan_assets}")

# Exportar relatÃ³rio
print(report.to_markdown())
```

**Formato do Query Log**:
```json
[
  {
    "query": "SELECT c.name, o.total FROM customers c JOIN orders o ON c.id = o.customer_id",
    "timestamp": "2024-12-15T10:30:00Z",
    "user": "analyst_maria",
    "duration_ms": 1250,
    "rows_scanned": 150000,
    "data_product": "customer_360"
  }
]
```

---

### 7. Sensitive Data NER Agent

Sistema de IA para **detecÃ§Ã£o e anonimizaÃ§Ã£o de dados sensÃ­veis** em texto livre, projetado como gateway de proteÃ§Ã£o para requisiÃ§Ãµes a LLMs terceiras.

**CaracterÃ­sticas**:
- DetecÃ§Ã£o de PII (CPF, CNPJ, SSN, email, telefone, nomes)
- DetecÃ§Ã£o de PHI (CID-10, CNS, CRM, prontuÃ¡rio mÃ©dico)
- DetecÃ§Ã£o de PCI (cartÃ£o de crÃ©dito, CVV, IBAN, SWIFT)
- DetecÃ§Ã£o de FINANCIAL (contas bancÃ¡rias, PIX, criptomoedas)
- DetecÃ§Ã£o de BUSINESS (projetos confidenciais, termos estratÃ©gicos)
- DetecÃ§Ã£o de CREDENTIALS (API keys, tokens, secrets, senhas)
- DetecÃ§Ã£o determinÃ­stica (90+ padrÃµes regex) + preditiva (heurÃ­sticas, checksum)
- **ğŸ†• IntegraÃ§Ã£o SpaCy**: NER com modelos treinados em portuguÃªs para maior precisÃ£o
- **ğŸ†• POS Tagging**: DistinÃ§Ã£o automÃ¡tica entre nomes prÃ³prios e verbos
- MÃºltiplas estratÃ©gias de anonimizaÃ§Ã£o (REDACT, MASK, HASH, PARTIAL, ENCRYPT)
- **Secure Vault**: Armazenamento criptografado AES-256 para mapeamentos originais/anonimizados
- **Controle de Acesso**: 5 nÃ­veis de permissÃ£o (READ_ONLY, DECRYPT, FULL_DECRYPT, ADMIN, SUPER_ADMIN)
- **PolÃ­tica de RetenÃ§Ã£o**: DELETE_ON_DECRYPT, RETAIN_DAYS, RETAIN_FOREVER
- **Audit Log**: Trilha de auditoria tamper-evident com hash chain

**ğŸ†• IntegraÃ§Ã£o SpaCy (Opcional)**:

O agente agora suporta SpaCy para maior precisÃ£o na detecÃ§Ã£o de nomes de pessoas, evitando falsos positivos como verbos e termos tÃ©cnicos.

```bash
# Instalar SpaCy e modelo portuguÃªs
pip install spacy
python -m spacy download pt_core_news_md  # modelo mÃ©dio (recomendado)
# ou
python -m spacy download pt_core_news_lg  # modelo grande (mais preciso)
```

Com SpaCy, o sistema:
- Classifica entidades como PER (Pessoa), ORG (OrganizaÃ§Ã£o), LOC (LocalizaÃ§Ã£o)
- Usa POS tagging para distinguir PROPN (nome prÃ³prio) de VERB (verbo)
- Evita falsos positivos como "Critical Error", "Carlos comprou", "Sistema Falhou"

**DocumentaÃ§Ã£o**: [sensitive_data_ner/README.md](sensitive_data_ner/README.md)

**Exemplo**:
```python
from sensitive_data_ner import SensitiveDataNERAgent, FilterPolicy, FilterAction

# Configurar polÃ­tica de filtro
policy = FilterPolicy(
    pii_action=FilterAction.ANONYMIZE,
    phi_action=FilterAction.BLOCK,
    pci_action=FilterAction.BLOCK,
    credentials_action=FilterAction.BLOCK,  # Bloquear API keys e secrets
    business_action=FilterAction.BLOCK,
)

agent = SensitiveDataNERAgent(
    filter_policy=policy,
    business_terms=["Projeto Arara Azul", "OperaÃ§Ã£o FÃªnix"]
)

# Analisar texto
text = """
O cliente JoÃ£o Silva, CPF 123.456.789-09, solicitou acesso.
Use a API key: sk-abc123xyz para autenticaÃ§Ã£o.
"""

result = agent.analyze(text)
print(f"Entidades: {result.statistics['total']}")
print(f"Risco: {result.risk_score:.1%}")
print(f"Texto seguro:\n{result.anonymized_text}")
```

**Com Secure Vault (armazenamento criptografado)**:
```python
from sensitive_data_ner import SecureVault, VaultConfig, RetentionPolicy

# Configurar vault com polÃ­tica de retenÃ§Ã£o
config = VaultConfig(
    storage_path=".secure_vault",
    default_retention_policy=RetentionPolicy.RETAIN_DAYS,
    default_retention_days=30,  # Manter por 30 dias apÃ³s decrypt
)

vault = SecureVault(config)

# Criar sessÃ£o e armazenar mapeamento
session = vault.create_session(
    user_id="user123",
    original_text=text,
    anonymized_text=result.anonymized_text,
    mappings=[{"original": "123.456.789-09", "anonymized": "[CPF]"}]
)

# Posteriormente: recuperar texto original (requer permissÃ£o DECRYPT)
original = vault.decrypt_session(
    session_id=session.session_id,
    user_id="admin_user"
)
```

**Apenas Data Classification Agent**:
```bash
pip install -r classification/requirements.txt
```

**Apenas Metadata Enrichment Agent**:
```bash
pip install -r metadata_enrichment/requirements.txt
export OPENAI_API_KEY="sua-chave-aqui"
```

---

## IntegraÃ§Ã£o entre Agentes

Os 7 agentes podem ser **integrados** para um framework completo de governanÃ§a:

```python
from lineage.data_lineage_agent import DataLineageAgent
from rag_discovery.agent import DataDiscoveryAgent
from metadata_enrichment.agent import MetadataEnrichmentAgent
from data_classification import DataClassificationAgent
from data_quality.agent import DataQualityAgent
from rag_discovery.providers.embeddings import SentenceTransformerEmbeddings
from rag_discovery.providers.llm import OpenAILLM
from rag_discovery.providers.vectorstore import ChromaStore

# 1. LINEAGE: Mapear dependÃªncias do pipeline
lineage_agent = DataLineageAgent()
lineage_result = lineage_agent.analyze_pipeline(["etl/*.sql", "etl/*.py"])
print(f"Assets mapeados: {lineage_result['metrics']['total_assets']}")

# 2. CLASSIFICATION: Classificar dados por sensibilidade
classification_agent = DataClassificationAgent()
classification_report = classification_agent.classify_from_csv("data/customers.csv")
print(f"Sensibilidade: {classification_report.overall_sensitivity}")
print(f"PII detectado: {classification_report.pii_columns}")
print(f"Compliance: {classification_report.compliance_flags}")

# 3. QUALITY: Avaliar qualidade dos dados
quality_agent = DataQualityAgent()
quality_report = quality_agent.evaluate_file(
    "data/customers.csv",
    freshness_config={"timestamp_column": "updated_at", "sla_hours": 24}
)
print(f"Qualidade: {quality_report.overall_score:.0%}")

# 4. ENRICHMENT: Gerar metadados automaticamente
enrichment_agent = MetadataEnrichmentAgent(
    embedding_provider=SentenceTransformerEmbeddings(),
    llm_provider=OpenAILLM(model="gpt-4o-mini"),
    vector_store=ChromaStore(collection_name="standards")
)
enrichment_result = enrichment_agent.enrich_from_csv("data/customers.csv")
print(f"DescriÃ§Ã£o: {enrichment_result.description}")

# 5. DISCOVERY: Indexar para busca semÃ¢ntica
discovery_agent = DataDiscoveryAgent(
    embedding_provider=SentenceTransformerEmbeddings(),
    llm_provider=OpenAILLM(),
    vector_store=ChromaStore(collection_name="catalog")
)

# Criar metadados enriquecidos com classificaÃ§Ã£o e qualidade
from rag_discovery.agent import TableMetadata
table = TableMetadata(
    name=enrichment_result.table_name,
    description=enrichment_result.description,
    columns=[{"name": c.name, "type": c.original_type, "description": c.description}
             for c in enrichment_result.columns],
    tags=enrichment_result.tags + [
        f"quality:{quality_report.overall_status}",
        f"sensitivity:{classification_report.overall_sensitivity}"
    ]
)
discovery_agent.index_metadata([table])

# Buscar com contexto completo
result = discovery_agent.discover("tabelas com dados de clientes e boa qualidade")
print(result.answer)
```

### Workflows Recomendados

| Workflow | Agentes | Caso de Uso |
|----------|---------|-------------|
| **CatalogaÃ§Ã£o AutomÃ¡tica** | Enrichment â†’ Discovery | Documentar data lake automaticamente |
| **AnÃ¡lise de Impacto** | Lineage + Discovery | Avaliar mudanÃ§as antes de deploy |
| **Compliance LGPD/GDPR** | Classification + Lineage | Rastrear e classificar dados pessoais |
| **DetecÃ§Ã£o de PII/PHI** | Classification | Identificar dados sensÃ­veis automaticamente |
| **Monitoramento ContÃ­nuo** | Quality + Discovery | Alertas de qualidade no catÃ¡logo |
| **Onboarding** | Discovery + Lineage | Entender o data lake rapidamente |
| **Valor de Ativos** | Value + Lineage | Identificar ativos crÃ­ticos e subutilizados |
| **PriorizaÃ§Ã£o de Investimentos** | Value + Classification | Focar em ativos de alto valor e alto risco |
| **Gateway de LLM Seguro** | NER + Vault | Filtrar dados sensÃ­veis antes de enviar a LLMs |
| **ProteÃ§Ã£o de Credenciais** | NER | Detectar vazamento de API keys e secrets |

---

## Interface Unificada (Streamlit)

O projeto inclui uma interface web unificada com todos os 7 agentes:

```bash
streamlit run app.py
```

**Tabs disponÃ­veis**:
- **Lineage**: Upload de arquivos e visualizaÃ§Ã£o de grafos
- **Discovery**: Chat com o catÃ¡logo de dados
- **Enrichment**: GeraÃ§Ã£o automÃ¡tica de metadados
- **Classification**: DetecÃ§Ã£o de PII/PHI/Financeiro
- **Quality**: AvaliaÃ§Ã£o de qualidade e alertas
- **Asset Value**: AnÃ¡lise de valor de ativos de dados
- **Sensitive Data NER**: DetecÃ§Ã£o e anonimizaÃ§Ã£o de dados sensÃ­veis com Vault

Cada agente tambÃ©m possui interface standalone:
```bash
streamlit run lineage/app.py          # Apenas Lineage
streamlit run rag_discovery/app.py    # Apenas Discovery (se disponÃ­vel)
streamlit run metadata_enrichment/streamlit_app.py  # Apenas Enrichment
streamlit run data_classification/streamlit_app.py  # Apenas Classification
streamlit run data_quality/streamlit_app.py         # Apenas Quality
streamlit run sensitive_data_ner/streamlit_app.py   # Apenas NER
```

---

## InstalaÃ§Ã£o

### PrÃ©-requisitos

- Python 3.8+
- OpenAI API Key (para agentes que usam LLM)
- [uv](https://docs.astral.sh/uv/) (recomendado) ou pip

### Instalando o uv

O `uv` Ã© um gerenciador de pacotes Python extremamente rÃ¡pido (escrito em Rust), que substitui pip, pip-tools e virtualenv.

```bash
# Linux/macOS
curl -LsSf https://astral.sh/uv/install.sh | sh

# Windows (PowerShell)
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"

# Via pip (alternativo)
pip install uv

# Via Homebrew (macOS)
brew install uv
```

### InstalaÃ§Ã£o Completa com uv (Recomendado)

```bash
# Clone o repositÃ³rio
git clone <repo-url>
cd data-governance-ai-agents-kit

# Crie e ative o ambiente virtual
uv venv
source .venv/bin/activate  # Linux/macOS
# .venv\Scripts\activate   # Windows

# Instale todas as dependÃªncias
uv pip install -r requirements.txt

# Configure variÃ¡veis de ambiente
export OPENAI_API_KEY="sua-chave-aqui"
```

### InstalaÃ§Ã£o Completa com pip

```bash
# Clone o repositÃ³rio
git clone <repo-url>
cd data-governance-ai-agents-kit

# Crie e ative o ambiente virtual
python -m venv .venv
source .venv/bin/activate  # Linux/macOS
# .venv\Scripts\activate   # Windows

# Instale todas as dependÃªncias
pip install -r requirements.txt

# Configure variÃ¡veis de ambiente
export OPENAI_API_KEY="sua-chave-aqui"
```

### InstalaÃ§Ã£o Individual

Instale apenas os agentes que vocÃª precisa:

**Com uv:**
```bash
# Apenas Lineage Agent (sem LLM)
uv pip install -r lineage/requirements.txt

# Apenas RAG Discovery Agent
uv pip install -r rag_discovery/requirements.txt

# Apenas Metadata Enrichment Agent
uv pip install -r metadata_enrichment/requirements.txt

# Apenas Data Classification Agent (sem LLM)
uv pip install -r data_classification/requirements.txt

# Apenas Data Quality Agent (sem LLM)
uv pip install -r data_quality/requirements.txt

# Apenas Sensitive Data NER Agent (sem LLM)
uv pip install -r sensitive_data_ner/requirements.txt
```

**Com pip:**
```bash
# Apenas Lineage Agent (sem LLM)
pip install -r lineage/requirements.txt

# Apenas RAG Discovery Agent
pip install -r rag_discovery/requirements.txt

# Apenas Metadata Enrichment Agent
pip install -r metadata_enrichment/requirements.txt

# Apenas Data Classification Agent (sem LLM)
pip install -r data_classification/requirements.txt

# Apenas Data Quality Agent (sem LLM)
pip install -r data_quality/requirements.txt

# Apenas Sensitive Data NER Agent (sem LLM)
pip install -r sensitive_data_ner/requirements.txt
```

---

## Arquitetura

```
data-governance-ai-agents-kit/
â”‚
â”œâ”€â”€ app.py                           # Interface Streamlit unificada (6 tabs)
â”œâ”€â”€ requirements.txt                 # DependÃªncias globais
â”œâ”€â”€ README.md                        # Este arquivo
â”‚
â”œâ”€â”€ lineage/                         # Data Lineage Agent
â”‚   â”œâ”€â”€ data_lineage_agent.py        # Agente principal
â”‚   â”œâ”€â”€ visualization_engine.py      # VisualizaÃ§Ãµes Plotly
â”‚   â”œâ”€â”€ parsers/                     # Parsers (SQL, Python, Terraform, etc.)
â”‚   â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ app.py                       # Streamlit standalone
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ rag_discovery/                   # Data Discovery RAG Agent
â”‚   â”œâ”€â”€ agent.py                     # Agente v2 (Dartboard Ranking)
â”‚   â”œâ”€â”€ data_discovery_rag_agent.py  # Agente v1 (compatibilidade)
â”‚   â”œâ”€â”€ providers/                   # Providers plugÃ¡veis
â”‚   â”‚   â”œâ”€â”€ embeddings/              # OpenAI, SentenceTransformers
â”‚   â”‚   â”œâ”€â”€ llm/                     # OpenAI, VertexAI
â”‚   â”‚   â””â”€â”€ vectorstore/             # ChromaDB, FAISS
â”‚   â”œâ”€â”€ retrieval/                   # Busca hÃ­brida
â”‚   â”œâ”€â”€ examples/
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ metadata_enrichment/             # Metadata Enrichment Agent
â”‚   â”œâ”€â”€ agent.py                     # Agente principal
â”‚   â”œâ”€â”€ standards/                   # RAG para normativos
â”‚   â”‚   â””â”€â”€ standards_rag.py
â”‚   â”œâ”€â”€ sampling/                    # Conectores de sampling
â”‚   â”‚   â””â”€â”€ data_sampler.py
â”‚   â”œâ”€â”€ providers/                   # Reusa rag_discovery/providers
â”‚   â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ streamlit_app.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ data_classification/             # Data Classification Agent
â”‚   â”œâ”€â”€ agent.py                     # Agente principal
â”‚   â”œâ”€â”€ classifiers/                 # Classificadores por categoria
â”‚   â”œâ”€â”€ rules/                       # Regras de classificaÃ§Ã£o
â”‚   â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ streamlit_app.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ data_quality/                    # Data Quality Agent
â”‚   â”œâ”€â”€ agent.py                     # Agente principal
â”‚   â”œâ”€â”€ metrics/                     # MÃ©tricas de qualidade
â”‚   â”‚   â”œâ”€â”€ quality_metrics.py       # 5 dimensÃµes
â”‚   â”‚   â””â”€â”€ schema_drift.py          # DetecÃ§Ã£o de drift
â”‚   â”œâ”€â”€ rules/                       # Sistema de regras
â”‚   â”‚   â””â”€â”€ quality_rules.py
â”‚   â”œâ”€â”€ connectors/                  # Conectores de dados
â”‚   â”‚   â””â”€â”€ data_connector.py
â”‚   â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ streamlit_app.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ data_asset_value/                # Data Asset Value Agent
â”‚   â”œâ”€â”€ agent.py                     # Agente principal com parser e calculator
â”‚   â”œâ”€â”€ __init__.py                  # Exports do mÃ³dulo
â”‚   â””â”€â”€ examples/
â”‚       â”œâ”€â”€ sample_query_logs.json   # Logs de queries de exemplo
â”‚       â”œâ”€â”€ data_products_config.json # Config de data products
â”‚       â”œâ”€â”€ asset_metadata.json      # Metadados de ativos (criticidade, custo, risco)
â”‚       â””â”€â”€ usage_example.py         # Script de exemplo de uso
â”‚
â””â”€â”€ sensitive_data_ner/              # Sensitive Data NER Agent
    â”œâ”€â”€ __init__.py                  # Exports principais
    â”œâ”€â”€ agent.py                     # SensitiveDataNERAgent (principal)
    â”œâ”€â”€ anonymizers.py               # EstratÃ©gias de anonimizaÃ§Ã£o
    â”œâ”€â”€ streamlit_app.py             # Interface visual
    â”œâ”€â”€ patterns/                    # PadrÃµes de detecÃ§Ã£o
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ entity_patterns.py       # 50+ padrÃµes regex por categoria
    â”œâ”€â”€ predictive/                  # DetecÃ§Ã£o preditiva
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ validators.py            # ValidaÃ§Ã£o de checksum (CPF, cartÃ£o, etc.)
    â”‚   â””â”€â”€ heuristics.py            # AnÃ¡lise de contexto e confianÃ§a
    â”œâ”€â”€ vault/                       # Armazenamento seguro
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ vault.py                 # SecureVault principal
    â”‚   â”œâ”€â”€ storage.py               # SQLite/PostgreSQL backends
    â”‚   â”œâ”€â”€ key_manager.py           # GestÃ£o de chaves AES-256
    â”‚   â”œâ”€â”€ access_control.py        # RBAC (5 nÃ­veis)
    â”‚   â””â”€â”€ audit.py                 # Audit logging tamper-evident
    â””â”€â”€ examples/
        â””â”€â”€ usage_example.py
```

---

## ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

```bash
# OpenAI (para RAG, Discovery e Enrichment)
export OPENAI_API_KEY="sk-..."
export OPENAI_API_URL="https://api.openai.com/v1"  # Opcional

# Modelo para Lineage (opcional)
export DATA_LINEAGE_LLM_MODEL="gpt-4o"

# Apache Atlas (opcional)
export ATLAS_HOST="http://atlas-host:21000"
export ATLAS_USERNAME="admin"
export ATLAS_PASSWORD="admin"
```

---

## ComparaÃ§Ã£o de Agentes

| CaracterÃ­stica | Lineage | Discovery | Enrichment | Classification | Quality | Asset Value | NER |
|---------------|---------|-----------|------------|----------------|---------|-------------|-----|
| **Objetivo** | Mapear dependÃªncias | Busca semÃ¢ntica | Gerar metadados | Classificar sensibilidade | Monitorar qualidade | Analisar valor | Anonimizar textos |
| **Input** | CÃ³digo (SQL, Python) | Query em LN | Dados (CSV, Parquet) | Dados (CSV, Parquet) | Dados (CSV, Parquet) | Query logs (JSON) | Texto livre |
| **Output** | Grafo + Impacto | Respostas + Tabelas | DescriÃ§Ãµes + Tags | PII/PHI/PCI + Compliance | Score + Alertas | Value Score + Insights | Texto anonimizado |
| **LLM** | Opcional | Requerido | Requerido | NÃ£o | NÃ£o | NÃ£o | NÃ£o |
| **Embeddings** | NÃ£o | Sim | Sim | NÃ£o | NÃ£o | NÃ£o | NÃ£o |
| **Principais Features** | Impact analysis, Ciclos | HÃ­brido search, RAG | Standards, Owner | LGPD, HIPAA, PCI-DSS | SLA, Schema drift | Usage, JOINs, Hubs | Vault, Audit, Retention |

---

## Casos de Uso

### 1. CatalogaÃ§Ã£o AutomÃ¡tica de Data Lake

```python
# Processar todos os arquivos do data lake
from pathlib import Path

for file in Path("data_lake/").glob("**/*.parquet"):
    # Avaliar qualidade
    quality = quality_agent.evaluate_file(str(file))

    # Enriquecer metadados
    enriched = enrichment_agent.enrich_from_parquet(str(file))

    # Indexar no catÃ¡logo
    discovery_agent.index_metadata([create_table_metadata(enriched, quality)])

print("CatÃ¡logo criado com metadados enriquecidos e scores de qualidade!")
```

### 2. Compliance LGPD/GDPR

```python
# Identificar e rastrear dados pessoais com Classification Agent
from data_classification import DataClassificationAgent

classifier = DataClassificationAgent()
results = []

for file in data_files:
    # Classificar dados por sensibilidade
    classification = classifier.classify_from_csv(file)

    if classification.pii_columns or classification.phi_columns:
        # Rastrear linhagem dos dados sensÃ­veis
        lineage = lineage_agent.analyze_pipeline([file])
        results.append({
            "file": file,
            "sensitivity": classification.overall_sensitivity,
            "pii_columns": classification.pii_columns,
            "phi_columns": classification.phi_columns,
            "compliance_flags": classification.compliance_flags,
            "downstream_impact": lineage["metrics"]["total_assets"]
        })

print(f"Encontrados {len(results)} arquivos com dados sensÃ­veis")
for r in results:
    print(f"  {r['file']}: {r['sensitivity']} - {r['compliance_flags']}")
```

### 3. Monitoramento de SLA

```python
# Verificar freshness diariamente
from data_quality.rules import QualityRule, AlertLevel

# Adicionar regra de SLA
agent.add_rule(QualityRule(
    name="orders_freshness_sla",
    dimension="freshness",
    table_name="orders",
    threshold=0.95,
    alert_level=AlertLevel.CRITICAL,
    params={"sla_hours": 4}
))

# Avaliar
report = agent.evaluate_file("orders.parquet")

# Verificar alertas
for alert in agent.get_active_alerts():
    print(f"[{alert.level}] {alert.message}")
```

---

## Roadmap

### ConcluÃ­do
- [x] Data Lineage Agent com mÃºltiplos parsers
- [x] Data Discovery RAG Agent com busca hÃ­brida
- [x] Metadata Enrichment Agent com PII detection
- [x] Data Classification Agent com LGPD/HIPAA/PCI-DSS
- [x] Data Quality Agent com SLA monitoring
- [x] Data Asset Value Agent com anÃ¡lise de uso e impacto
- [x] Sensitive Data NER Agent com detecÃ§Ã£o de 6 categorias (PII, PHI, PCI, Financial, Business, Credentials)
- [x] Secure Vault com criptografia AES-256 e controle de acesso
- [x] PolÃ­tica de retenÃ§Ã£o (DELETE_ON_DECRYPT, RETAIN_DAYS, RETAIN_FOREVER)
- [x] Interface Streamlit unificada (7 agentes)
- [x] IntegraÃ§Ã£o com Apache Atlas
- [x] Providers plugÃ¡veis (embeddings, LLM, vectorstore)

### Em Desenvolvimento
- [ ] Column-level lineage
- [ ] IntegraÃ§Ã£o com dbt
- [ ] IntegraÃ§Ã£o com AWS Glue Data Catalog
- [ ] IntegraÃ§Ã£o com Databricks Unity Catalog
- [ ] API REST para integraÃ§Ã£o com outras ferramentas
- [ ] Dashboard de mÃ©tricas de governanÃ§a
- [ ] Suporte a modelos locais (Ollama)

### Classification Agent
- [x] Regras de PII/PHI/Financeiro baseadas em metadados
- [x] NÃ­veis de severidade e recomendaÃ§Ãµes LGPD/GDPR
- [ ] ValidaÃ§Ã£o multilÃ­ngue com LLM
- [ ] Biblioteca ampliada de regras setoriais

### Metadata Enrichment Agent
- [x] RAG sobre normativos internos
- [x] Suporte a sampling (CSV, Parquet, SQL, Delta)
- [x] ExportaÃ§Ã£o em JSON/Markdown/HTML
- [ ] Conectores adicionais (BigQuery, S3 inventories)
- [ ] Templates personalizÃ¡veis de catÃ¡logo

---

## Contribuindo

ContribuiÃ§Ãµes sÃ£o muito bem-vindas! Este guia explica passo a passo como contribuir com o projeto.

### PrÃ©-requisitos

Antes de contribuir, certifique-se de ter:
- Conta no GitHub
- Git instalado localmente
- Python 3.8+ instalado
- [uv](https://docs.astral.sh/uv/) instalado (recomendado)

### Passo 1: Fazer Fork do RepositÃ³rio

1. Acesse o repositÃ³rio original no GitHub
2. Clique no botÃ£o **"Fork"** no canto superior direito
3. Selecione sua conta como destino do fork
4. Aguarde a criaÃ§Ã£o do fork (cÃ³pia do repositÃ³rio na sua conta)

### Passo 2: Clonar seu Fork

```bash
# Clone SEU fork (substitua <seu-usuario> pelo seu username do GitHub)
git clone https://github.com/<seu-usuario>/data-governance-ai-agents-kit.git

# Entre no diretÃ³rio
cd data-governance-ai-agents-kit

# Configure o repositÃ³rio original como "upstream"
git remote add upstream https://github.com/allpedroza/data-governance-ai-agents-kit.git

# Verifique os remotes configurados
git remote -v
# Deve mostrar:
# origin    https://github.com/<seu-usuario>/data-governance-ai-agents-kit.git (fetch)
# origin    https://github.com/<seu-usuario>/data-governance-ai-agents-kit.git (push)
# upstream  https://github.com/allpedroza/data-governance-ai-agents-kit.git (fetch)
# upstream  https://github.com/allpedroza/data-governance-ai-agents-kit.git (push)
```

### Passo 3: Manter seu Fork Atualizado

Antes de criar uma nova branch, sempre sincronize com o repositÃ³rio original:

```bash
# Busque as atualizaÃ§Ãµes do repositÃ³rio original
git fetch upstream

# Mude para a branch principal
git checkout main

# Mescle as atualizaÃ§Ãµes
git merge upstream/main

# Atualize seu fork no GitHub
git push origin main
```

### Passo 4: Criar uma Branch para sua ContribuiÃ§Ã£o

**Importante**: Nunca faÃ§a commits diretamente na branch `main`. Sempre crie uma branch especÃ­fica.

```bash
# Crie e mude para uma nova branch
git checkout -b feature/nome-da-sua-feature

# Exemplos de nomes de branch:
# feature/add-bigquery-connector     (para novas funcionalidades)
# fix/lineage-parser-error           (para correÃ§Ãµes de bugs)
# docs/update-readme                 (para documentaÃ§Ã£o)
# refactor/improve-quality-metrics   (para refatoraÃ§Ãµes)
```

### Passo 5: Fazer suas AlteraÃ§Ãµes

1. FaÃ§a as alteraÃ§Ãµes necessÃ¡rias no cÃ³digo
2. Adicione ou atualize testes se aplicÃ¡vel
3. Verifique se o cÃ³digo segue os padrÃµes do projeto
4. Teste suas alteraÃ§Ãµes localmente

```bash
# Configure o ambiente
uv venv
source .venv/bin/activate
uv pip install -r requirements.txt

# Execute os testes (se houver)
pytest

# Verifique se a aplicaÃ§Ã£o funciona
streamlit run app.py
```

### Passo 6: Commit das MudanÃ§as

```bash
# Adicione os arquivos modificados
git add .

# FaÃ§a o commit com mensagem descritiva
git commit -m "feat: adiciona conector para BigQuery"

# PadrÃµes de mensagem de commit:
# feat: nova funcionalidade
# fix: correÃ§Ã£o de bug
# docs: alteraÃ§Ãµes em documentaÃ§Ã£o
# refactor: refatoraÃ§Ã£o de cÃ³digo
# test: adiÃ§Ã£o ou modificaÃ§Ã£o de testes
# chore: tarefas de manutenÃ§Ã£o
```

### Passo 7: Enviar para seu Fork

```bash
# Envie sua branch para seu fork no GitHub
git push origin feature/nome-da-sua-feature
```

### Passo 8: Criar um Pull Request

1. Acesse seu fork no GitHub
2. VocÃª verÃ¡ uma mensagem sugerindo criar um Pull Request - clique em **"Compare & pull request"**
3. Ou clique na aba **"Pull requests"** â†’ **"New pull request"**
4. Certifique-se de que:
   - **base repository**: `allpedroza/data-governance-ai-agents-kit`
   - **base**: `main`
   - **head repository**: `<seu-usuario>/data-governance-ai-agents-kit`
   - **compare**: `feature/nome-da-sua-feature`
5. Preencha o tÃ­tulo e descriÃ§Ã£o do PR:
   - Descreva o que foi alterado
   - Explique o motivo da alteraÃ§Ã£o
   - Mencione issues relacionadas (se houver)
6. Clique em **"Create pull request"**

### Boas PrÃ¡ticas

- **Mantenha PRs pequenos e focados**: Um PR deve resolver uma Ãºnica issue ou adicionar uma Ãºnica feature
- **Escreva testes**: Sempre que possÃ­vel, adicione testes para novas funcionalidades
- **Documente**: Atualize a documentaÃ§Ã£o se sua alteraÃ§Ã£o afetar o uso do projeto
- **Siga o estilo do cÃ³digo**: Mantenha consistÃªncia com o cÃ³digo existente
- **Responda feedback**: Esteja disponÃ­vel para responder comentÃ¡rios e fazer ajustes

### Fluxo Visual

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FLUXO DE CONTRIBUIÃ‡ÃƒO                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. FORK                    2. CLONE                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚  â”‚ Original â”‚â”€â”€forkâ”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Seu Fork â”‚                         â”‚
â”‚  â”‚  (GitHub)â”‚              â”‚ (GitHub) â”‚                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                 â”‚clone                          â”‚
â”‚                                 â–¼                               â”‚
â”‚                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚  3. BRANCH                 â”‚  Local   â”‚                         â”‚
â”‚  main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Repo   â”‚                         â”‚
â”‚    â””â”€â”€ feature/xyz         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                 â”‚                               â”‚
â”‚  4. COMMIT & PUSH               â”‚                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚  â”‚                                                              â”‚
â”‚  â–¼                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    push     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    PR     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Local   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Seu Fork â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Original â”‚  â”‚
â”‚  â”‚  Branch  â”‚             â”‚ (GitHub) â”‚           â”‚ (GitHub) â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  5. REVIEW: Aguarde revisÃ£o e faÃ§a ajustes se necessÃ¡rio       â”‚
â”‚  6. MERGE: ApÃ³s aprovaÃ§Ã£o, sua contribuiÃ§Ã£o serÃ¡ incorporada!  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### DÃºvidas?

Se tiver dÃºvidas sobre como contribuir, abra uma [issue](https://github.com/allpedroza/data-governance-ai-agents-kit/issues) no repositÃ³rio.

---

## LicenÃ§a

Este projeto estÃ¡ licenciado sob a licenÃ§a MIT - veja o arquivo LICENSE para detalhes.

---

## Agradecimentos

- **Apache Atlas** - CatÃ¡logo de metadados
- **ChromaDB** - Banco vetorial
- **OpenAI** - Embeddings e LLM
- **SentenceTransformers** - Embeddings locais
- **NetworkX** - AnÃ¡lise de grafos
- **Plotly** - VisualizaÃ§Ãµes interativas
- **Streamlit** - Interface web
